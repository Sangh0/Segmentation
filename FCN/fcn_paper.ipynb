{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f0f04a",
   "metadata": {},
   "source": [
    "# Fully Convolutional Networks for Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac9a27",
   "metadata": {},
   "source": [
    "## Abstract  \n",
    "- Convolutional Networks are trained end-to-end, pixels-to-pixels.  \n",
    "- Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output.  \n",
    "- We adapt classification networks (AlexNet, VGG, GoogLeNet) into FCN and transfer their learned representations by fine-tuning to the segmentation task.  \n",
    "- And we define a skip architecture that combines sementic information from a deep, coarse layer with appearance information from a shallow, fine layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97aaf8",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "<img src = 'https://appsilondatascience.com/assets/uploads/2018/08/types.png' width=1000>  \n",
    "\n",
    "- Convolutional networks are driving advances in recognizition.  \n",
    "- Convnets are note only improving for image classification, but also object detection.  \n",
    "- The natural next step in the progression from coarse to fine inference is to make a prediction at every pixel.  \n",
    "  \n",
    "  \n",
    "- We show that a FCN trained end-to-end, pixels-to-pixels on semantic segmentation.  \n",
    "    - 1. pixelwise prediction  \n",
    "    - 2. supervised pre-training  \n",
    "      \n",
    "      \n",
    "- Fully convolutional versions of existing networks predict dense outputs from arbitrary-sized inputs.  \n",
    "- In this paper, we use upsampling method because the output has to be the same size as the input.  \n",
    "\n",
    "\n",
    "- Semantic segmentation faces an inherent tension between semantics and locations: global information resolves what while local information resolves where.  \n",
    "- We define a skip architecture to take advantage of this feature spectrum that combines deep, coarse, semantic information and shallow, fine, appearance information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3dc4b",
   "metadata": {},
   "source": [
    "## Receptive Fields and Loss  \n",
    "- Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their **receptive fields**.  \n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnUreZ%2Fbtq2SNdVdQh%2FewTMikTKKKpnSJG87YNkH0%2Fimg.png\">  \n",
    "\n",
    "#### Loss  \n",
    "$$\\mathscr{l}\\left(\\mathbf{x};\\theta\\right)=\\sum_{ij}\\mathscr{l}'\\left(\\mathbf{x}_{ij};\\theta\\right)$$  \n",
    "- If the loss function is sum over the spatial dimensions of the final layer, its gradient will be a sum over the gradients of each of its spatial components.  \n",
    "- Thus SGD on $\\mathscr{l}$ computed on whole images will be the same as SGD on $\\mathscr{l}'$, taking all of the final layer receptive fields as a minibatch.  \n",
    "\n",
    "#### Computing efficiency  \n",
    "- When these receptive fields overlap significantly, both feedforward computation and backpropagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1339025",
   "metadata": {},
   "source": [
    "## Adapting classifiers for dense prediction  \n",
    "- Typical recognition nets, including LeNet, AlexNet, and its deeper successors, ostensibly take fixed-sized inputs and produce non-spatial outputs.  \n",
    "- The fully connected layers of these nets have fixed dimensions and throw away spatial coordinates.  \n",
    "- However, these fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions.  \n",
    "- Doing so casts them into fully convolutional networks that take input of any size and output classification maps.\n",
    "\n",
    "\n",
    "<img src = \"https://mblogthumb-phinf.pstatic.net/MjAxNzAzMTRfMTg5/MDAxNDg5NDkwNjAxNzI1.ePM0OvxwEyG7lIBciOLyF75YZ0z5Mq8SDwcNlI6pOUEg.MqEmYMEAQhwyCnt2iszdO0XLnDgAeiHPSZc4DzUmjFog.PNG.laonple/%EC%9D%B4%EB%AF%B8%EC%A7%80_15.png?type=w2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deafabd",
   "metadata": {},
   "source": [
    "## Shift-and-stitch is filter rarefaction  \n",
    "- Dense predictions can be obtained from coarse outputs by stitching together output from shifted versions of the input.  \n",
    "- In semantic segmentation, we proceed upsampling because the output size must be the same as the input.  \n",
    "- If upsampling is simply performed, the output image has a lower resolution than the input image.  \n",
    "- So, we introduce shift-and-stitch.  \n",
    "\n",
    "<img src = \"https://image.slidesharecdn.com/semanticsegmentationslides-181218144057/95/semantic-segmentation-fully-convolutional-networks-for-semantic-segmentation-12-638.jpg?cb=1545144119\">  \n",
    "\n",
    "- First, the red box shows a general convolutional operation.  \n",
    "- The yellow box performs the same operation as the red box, but the difference is the input matrix.  \n",
    "- Looking at the result, the right side is gray color, which is unnecessary information because the input image was moved by one pixel to the left.  \n",
    "- If you repeat above process several times, we get the final result as shown in the last figure.  \n",
    "\n",
    "**problem**  \n",
    "- This method has the disadvantage of high conputational costs.  \n",
    "- We find that the skip layer fusion is a more efficient method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd623c",
   "metadata": {},
   "source": [
    "## Upsampling is backwards strided convolution  \n",
    "- We introduce two method to obtain the spatial output.  \n",
    "     1. **Interpolation**  \n",
    "     <img src = \"https://media.vlpt.us/images/kimkj38/post/cd214401-867e-4538-b5a7-586d2223c0e7/image.png\">  \n",
    "     <img src = \"https://media.vlpt.us/images/kimkj38/post/75c0a268-3201-4867-8ee9-aa3541034191/image.png\">  \n",
    "     \n",
    "         - In this way, when expanding a low resolution image to a high resolution image, empty values can be inferred and filled.  \n",
    "     2. **Deconvolution**  \n",
    "     <img src = \"https://blog.kakaocdn.net/dn/5EvJx/btqSxBHlTCL/ElN9OMvxt2WGuhlY0vFk60/img.png\">  \n",
    "     - This operation is trivial to implement, since it simply reverses the forward and backward passes of convolution.  \n",
    "     - In the above figure, an upsampled output can be obtained by adding overlapping regions.   \n",
    "     \n",
    "- In our experiments, we find that in-network upsampling is fast and effective for learning dense prediction.   \n",
    "- But, information loss is still large because both methods estimate from the feature map that the image is greatly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ecf2a",
   "metadata": {},
   "source": [
    "## Combining what and where  \n",
    "**Add skip layers**\n",
    "- We define a new fully convolutional net (FCN) for segmentation.  \n",
    "- While fully convolutionalized classifiers can be fine-tuned to segmentation, and even score highly on the standard metric, their output is dissatisfyingly coarse.  \n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FVceIj%2Fbtq3ssThmLV%2FykrKdZFpn8F59Ik8sAeqFK%2Fimg.png\">  \n",
    "<img src = \"https://media.vlpt.us/images/sanha9999/post/81948460-c24f-49db-abd2-096340febb99/FCN.png\">  \n",
    "\n",
    "- The 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output.  \n",
    "- We address this by adding skip that combine the final prediction layer with lower layers with finer strides.  \n",
    "- Through the conv+pool layers several times, the details of the features disappear.  \n",
    "- So, we fill the details using the feature map in front of the last layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fb72b",
   "metadata": {},
   "source": [
    "## Patchwise training is loss sampling  \n",
    "**patchwise learning**  \n",
    "- step 1. Set a patch of a specific size and input it to the CNN.  \n",
    "- step 2. classification by CNN.  \n",
    "- step 3. The pixels located in the center of its patch are classified into corresponding class.  \n",
    "- step 4. Repeat this process in a sliding window method.\n",
    "<img src = \"https://media.vlpt.us/images/leejaejun/post/bd363d37-7a14-4e7e-8bcf-ab49e8e03e06/image.png\" width=800>  \n",
    "\n",
    "- When learning in the patchwise method, this may be overlapping patrs between patches.  \n",
    "- So, the unnecessary computation increases.   \n",
    "- We do not find that it yields faster or better convergence for dense prediction.    \n",
    "- Whole image training is effective and efficient.  \n",
    "\n",
    "**Patch Sampling**  \n",
    "- We find that sampling does not have a significant effect on convergence rate compared to whole image training, but takes significantly more time due to the larger number of images that need to be considered per batch.  \n",
    "- We therefore choose unsampled, whole image training in our other experiments.\n",
    "\n",
    "<img src = \"https://media.vlpt.us/images/qsdcfd/post/f0b1b648-e9cd-4235-96aa-7f6e0913ecb5/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eec32f",
   "metadata": {},
   "source": [
    "## From classifier to dense FCN   \n",
    "**fully connected layer $\\rightarrow$ fully convolutional layer**\n",
    "- We pick the VGG16 net among AlexNet, GoogLeNet, VGG nets.  \n",
    "- We decapitate each net by discarding the final classifier layer, and convert all fully connected layers to convolutions.  \n",
    "- We append a $1\\times 1$ convolution with channel dimension 21 to predict scores for each of the PASCAL classes at each of the coarse output locations, followed by a deconvolution layer to bilinearly upsample the coarse outputs to pixel-dense outputs.  \n",
    "\n",
    "<img src = \"https://khyeyoon.github.io/assets/img/FCN/T1.PNG\">  \n",
    "\n",
    "- VGG 16 performed the best compared to 3 networks.  \n",
    "- Despite similar classification accuracy, our implementation of GoogLeNet did not match the VGG 16 segmentation result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcd858",
   "metadata": {},
   "source": [
    "## Experimental framework  \n",
    "**Optimization**  \n",
    "- We train by SGD with momentum.  \n",
    "- We use a minibatch size of 20 images and fixed learning rates of $10^{-3}, 10^{-4},$ and $5^{-5}$.  \n",
    "- We use momentum 0.9, weight decay of $5^{-4}$ or $2^{-4}$, and doubled learning rate for biases, although we found training to be sensitive to the elarning rate alone. \n",
    "\n",
    "**Fine-tuning**  \n",
    "- We fine-tune all layers by backpropagation through the whold net.  \n",
    "- Training from scratch is not feasible considering the time required to learn the base classification nets.  \n",
    "- So, we use transfer learning with pre-trained CNN.  \n",
    "\n",
    "**Class Balancing**  \n",
    "- Fully convolutional training can balance classes by weighting or sampling the loss.  \n",
    "- We find class balancing unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a8bc3",
   "metadata": {},
   "source": [
    "## Results  \n",
    "- We test our FCN on semantic segmentation and scene parsing, exploring PASCAL VOC, NYUDv2, and SIFT Flow.  \n",
    "\n",
    "**Metrics**  \n",
    "- pixel accuracy : $\\sum_{i}n_{ii}/ \\sum_{i}t_{i}$  \n",
    "- mean accuracy : $\\left(1/n_{cl}\\right)\\sum_{i}n_{ii}/t_{i}$  \n",
    "- mean IU : $\\left(1/n_{cl}\\right)\\sum_{i}n_{ii}/\\left(t_{i}+\\sum_{j}n_{ji}-n_{ii}\\right)$  \n",
    "- frequency weighted IU : $\\left(sum_kt_k\\right)^{-1}\\sum_it_in_{ii}/\\left(t_i+\\sum_jn_{ji}-n_{ii}\\right)$  \n",
    "\n",
    "**Compare to FCN** $\\qquad\\qquad\\qquad\\qquad\\qquad$ **NYUDv2** $\\qquad\\qquad\\qquad\\qquad\\qquad$ **SIFT Flow**\n",
    "<img src = \"https://miro.medium.com/max/2000/1*2obgSShyzzBKuds_XxPCoA.png\">  \n",
    "\n",
    "**$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$PASCAL VOC**\n",
    "<img src = \"https://t1.daumcdn.net/cfile/tistory/99268446605C18E311?original\">\n",
    " \n",
    "<img src = \"https://cdn-images-1.medium.com/max/1600/1*fU9El2B2qELtKsD9FsHB-w.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e1569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
