{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a872127d",
   "metadata": {},
   "source": [
    "# PIDNet (A Real-time Semantic Segmentation Network Inspired from PID Controller)  \n",
    "- 논문 리뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7041b4",
   "metadata": {},
   "source": [
    "## Abstract  \n",
    "- Two-branch network는 real-time semantic segmentation에서 효과적인 성능을 보여줬다  \n",
    "- 그러나 low-level detail과 high-level semantic feature를 바로 결합하면 detail feature가 contexual information에 의해 쉽게 압도당한다  \n",
    "- 우리는 이를 **overshoot**이라 한다  \n",
    "- 본 논문에서 PID controller라는 개념을 이용해 overshoot issue에서 벗어났다는 것을 밝힌다   \n",
    "- 우리는 새로운 3개의 Branch network를 제안한다  \n",
    "- PIDNet은 detail, context 그리고 boundary information을 가지고 있으며 final stage에서 detail과 context branch의 fusion을 가이드하기 위해 boundary attention을 사용한다  \n",
    "- PIDNet은 inference speed와 정확도 사이의 trade-off를 극복했다  \n",
    "- 특히 Cityscapes test set에서 93.2 FPS와 78.6%의 mIoU를 달성했다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4f5e3",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure1.JPG?raw=true\" width=700>\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure2.JPG?raw=true\" width=700>\n",
    "\n",
    "- PID controller는 지난 세기 제안된 전통적인 concept로 로봇 조작, 화학 공정, 전력 시스템과 같은 프로세스에 널리 적용되어 왔다  \n",
    "- 예를 들어 PID controller의 근본적인 방법론으로 image denoising, SGD, optimization이 있으며 이들은 기존 method들에 비해 크게 개선되었다  \n",
    "- 본 논문에서 PID controller의 개념을 활용한 PIDNet을 소개한다  \n",
    "- PIDNet은 이전의 SOTA 모델을 능가하며 best trade-off를 달성했다  \n",
    "- semantic segmentation model은 중요한 detailed information 손실 없이 픽셀 사이의 context 종속성을 학습할 수 있는 능력을 갖춰야 한다  \n",
    "- 즉, large receptive field를 얻을 수 있어야 함  \n",
    "- 성능이 굉장히 좋아도 많은 공간과 시간 복잡성으로 인해 자율 주행과 같은 real-time task에는 적용을 못했다  \n",
    "- real-time이나 mobile requirement를 만족시키기 위해 researcher들은 효율적인 모델들을 제안했다  \n",
    "- 특히 ENet은 초기 stage에서 feature map을 down-sampling하고 lightweight decoder을 적용해 inference speed를 개선시켰다  \n",
    "- MobileNetv2는 전통적인 모델의 복잡성을 줄이기 위해 convolution 대신 depthwise separable convolution을 사용했고 regularization 효과를 위해 inverted residual block을 제안했다  \n",
    "- 이러한 초기 연구들로 인해 segmentation model의 대기 시간과 메모리 사용량을 줄이게 됐지만 낮은 정확도로 인해 real-time application에 제한이 있었다  \n",
    "- 최근 몇몇 성공적인 모델들이 two branch 기반 모델을 제안하면서 SOTA를 달성했다  \n",
    "- 이 논문에서 우리는 PID controller로부터 two branch network를 깊게 분석하고 어떻게 overshoot issue에서 벗어났는지 주목한다  \n",
    "- 우리는 boundary detection이라는 하나의 branch를 더 갖춘 새로운 three branch network인 PIDNet을 제안한다  \n",
    "- Cityscapes, CamVid, COCO stuff dataset에서 SOTA를 달성했다  \n",
    "\n",
    "- **main contribution**  \n",
    "    - 1. PID controller 기반 three branch network를 제안한다  \n",
    "    - 2. PIDNet의 성능을 높이기 위해 fast context aggregation module과 boundary-guided fusion module을 제안한다  \n",
    "    - 3. 우리 모델은 best trade-off를 달성하며 Cityscapes test set에서 78.6%의 mIoU와 93.2 FPS를 자랑한다  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359db6c",
   "metadata": {},
   "source": [
    "## Method  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure3.JPG?raw=true\" width=700>\n",
    "\n",
    "- PID controller는 상호보완적인 능력을 가지는 3개의 요소로 구성되어 있다  \n",
    "- (P) Proportional controller는 현재의 error를 제공하고  \n",
    "- (I) Integral controller는 이전의 error를 축적하고  \n",
    "- (D) Derivative controller는 미래에 바뀔 error를 예측한다  \n",
    "- 따라서 PID controller의 output은 전체 시간 domain의 error에 기반에 생성된다  \n",
    "- PI controller는 대부분의 setpoint control scenario를 만족시키지만 overshoot issue에 시달린다  \n",
    "- 더 dynamic한 반응을 위해 Derivative contorller를 소개하며 이는 overshoot이 일어나기 전에 output을 조정하며 예측할 수 있다  \n",
    "- two-branch network에서 context branch는 픽셀 간의 종속성을 분석하기 위해 strided convolution이나 pooling을 cascade함으로써 local에서 global 영역으로 semantic information을 종합한다  \n",
    "- detail branch는 각 픽셀에 대한 semantic과 localization 정보를 보존하기 위해 high resolution feature map을 유지한다  \n",
    "- 따라서 detail과 context branch는 spatial domain에서 Proportional과 Integral controller로 볼 수 있다  \n",
    "\n",
    "### PIDNet: A Novel Three-branch Network  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure4.JPG?raw=true\">\n",
    "\n",
    "- overshoot issue를 완화하기 위해 우리는 two-branch network에 Auxiliary Derivative Branch를 제공하며 spatial domain에서 PID controller를 모방하는 것을 제안한다  \n",
    "- 각 object 내부의 픽셀에 대한 semantic은 일관적이며 인접 boundary는 일관적이지 않다  \n",
    "- 따라서 semantic의 derivative는 object boundary에서만 nonzero이고 ADB function은 boundary detection을 수행해야 한다  \n",
    "- 따라서 본 논문에서 three branch real-time semantic segmentation인 PIDNet을 제안한다  \n",
    "- P : high resolution feature map을 분석하고 보존  \n",
    "- I : long range dependency를 분석하기 위해 locally하고 globally한 information을 결합  \n",
    "- D : boundary region을 예측하기 위해 high-frequency feature를 추출  \n",
    "- P, I, D branch의 깊이는 효율적인 구현을 위해 deep하고 shallow하게 구성되어 있다  \n",
    "- 우리는 더 나은 최적화를 위해 추가의 semantic loss $l_0$를 설정하며 이를 위해 첫 번째 Pag module의 output에 semantic head를 배치한다  \n",
    "- Dice loss 대신에 weighted binary cross entropy loss $l_1$이 적용되며 이는 boundary detection의 imbalanced problem을 다루기 위해서다  \n",
    "- $l_2$와 $l_3$는 CE loss를 사용하며 semantic segmentation과 boundary detection을 조정하기 위해 boundary head의 output을 사용한 $l_3$에 대해서는 boundary-awareness CE loss를 활용한다  \n",
    "\n",
    "$$l_3=-\\sum_{i,c}\\left\\{1: b_i > t\\right\\}\\left(s_{i,c}\\log \\hat{s}_{i,c}\\right)$$  \n",
    "\n",
    "- 이때 $t$는 threshold를 의미하며 $b_i$, $s_{i,c}$, $\\hat{s}_{i,c}$는 각각 boundary head, segmentation ground truth 그리고 예측 결과를 의미한다  \n",
    "- $i$는 픽셀, $c$는 클래스  \n",
    "- 따라서 최종 loss function은 다음 식과 같다  \n",
    "\n",
    "$$Loss = \\lambda_0l_0+\\lambda_1l_1+\\lambda_2l_2+\\lambda_3l_3$$  \n",
    "\n",
    "- 본 논문에서는 $\\lambda_0=0.4$, $\\lambda_1=20$, $\\lambda_2=1$, $\\lambda_3=1$ 그리고 $t=0.8$로 설정했을 때 성능이 가장 좋았다    \n",
    "\n",
    "### Pag: Selective Learning High-level Semantics  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure5.JPG?raw=true\" width=700>\n",
    "\n",
    "- PIDNet에서 I branch에서 얻은 풍부하고 정확한 semantic information은 P branch의 detail parsing에 중요하다  \n",
    "- 이때 P branch는 비교적 적은 레이어와 채널을 가진다  \n",
    "- 따라서 I branch는 다른 두 branch에 대한 backup으로 여기며 이들에게 필요한 정보를 제공한다  \n",
    "- 제공 받은 feature map을 바로 더해주는 D branch와 달리 P branch는 유용한 semantic information을 선택적으로 학습한다  \n",
    "- 우리는 이것이 가능하게 도와주는 Pixel-attention-guided fusion module (Pag)를 소개한다  \n",
    "- Pag의 concept은 self-attention mechanisn에서 가져왔다  \n",
    "- 그러나 real-time requirement를 위해 attention을 locally로 계산한다  \n",
    "- P branch의 output feature를 $\\textbf{v}_p$, I branch의 output feature를 $\\textbf{v}_i$로 정의하며 $\\sigma$는 다음 식과 같이 정의한다  \n",
    "\n",
    "$$\\sigma = Sigmoid\\left(f_p\\left(\\textbf{v}_p\\right)\\cdot f_i\\left(\\textbf{v}_i\\right)\\right)$$  \n",
    "\n",
    "- 이때 $\\sigma$는 같은 object에서 나올 가능성을 나타낸다  \n",
    "- 만약 $\\sigma$가 높은 값을 가지면 I branch가 semantically하게 정확하기 때문에 $\\textbf{v}_i$를 더 신뢰하며 그 반대도 마찬가지이다  \n",
    "- 따라서 Pag module의 output은 다음 식과 같이 쓸 수 있다  \n",
    "\n",
    "$$Out_{Pag}=\\sigma\\textbf{v}_i + \\left(1-\\sigma\\right)\\textbf{v}_p$$  \n",
    "\n",
    "### PAPPM: Fast Aggregation of Contexts  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure6.JPG?raw=true\" width=700>\n",
    "\n",
    "- DAPPM(Deep Aggregation PPM)은 PPM의 output embedding 능력을 개선했으며 우수한 성능을 보여준다  \n",
    "- 그러나 DAPPM은 병렬적으로 처리를 못하며 각 scale마다 너무 많은 채널을 가지고 있다  \n",
    "- 따라서 우리는 figure 6처럼 DAPPM을 병렬로 처리할 수 있게 변경하고 각 scale마다 채널 수를 128에서 96으로 줄인다  \n",
    "- 이 새로운 Context harvesting module은 Parrel Aggregation PPM (PAPPM)이라 칭하며 PIDNet-M과 PIDNet-S에 적용된다  \n",
    "- deep model PIDNet-L에 대해서는 DAPPM을 적용하지만 각 scale마다 채널 수를 128에서 112로 줄인다  \n",
    "\n",
    "### Bag: Balancing the Details and Contexts  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure7.JPG?raw=true\" width=700>\n",
    "\n",
    "- ADB에서 추출된 boundary feature가 주어지면 우리 proposal은 detailed P와 context I representation의 결합을 가이드하기 위해 boundary attention을 사용한다  \n",
    "- 따라서 우리는 3개 branch의 결합을 위해 Boundary-attention-guided fusion module (Bag)을 설계했다  \n",
    "- context branch는 sematically rich하며 정확한 semantic을 제공하지만 boundary region과 작은 object에 대한 spatial과 geometric detail이 부족하다  \n",
    "- 여기서 부족한 spatial detail은 detail branch를 통해 해결한다  \n",
    "- P, I, D branch의 output을 각각 $\\textbf{v}_p$, $\\textbf{v}_i$, $\\textbf{v}_d$라고 정의하며 sigmoid 함수를 적용한 Bag와 Light-Bag은 다음과 같이 쓴다  \n",
    "\n",
    "$$\\sigma = Sigmoid\\left(\\textbf{v}_d\\right)$$  \n",
    "$$Out_{bag}=f_{out}\\left(\\left(1-\\sigma\\right)\\bigotimes\\textbf{v}_i + sigmoid \\bigotimes \\textbf{v}_p\\right)$$  \n",
    "$$Out_{light}=f_p\\left(\\left(1-\\sigma\\right)\\bigotimes\\textbf{v}_i+\\textbf{v}_ip\\right) + f_i\\left(\\sigma\\bigotimes\\textbf{v}_p+\\textbf{v}_i\\right)$$  \n",
    "\n",
    "- 함수 $f$는 Conv + BN + ReLU를 의미하고 Light-Bag에서는 $3\\times 3$ 대신 $1\\times 1$ conv를 적용한다  \n",
    "- 만약 $\\sigma > 0.5$이면 detailed feature를 더 신뢰하고 아니면 context information을 더 선호한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caddceb",
   "metadata": {},
   "source": [
    "## Experiment  \n",
    "### Datasets  \n",
    "**Cityscapes**  \n",
    "- train: 2975, valid: 500, test: 1525  \n",
    "- The annotation contains 30 classes but only 19 of them  \n",
    "- resolution: $2048\\times 1024$  \n",
    "\n",
    "**CamVid**  \n",
    "- train: 367, valid: 101, test: 233  \n",
    "- The number of annotated categories is 32, of which 11 classes are used  \n",
    "- resolution: $960\\times 720$  \n",
    "\n",
    "**COCO-Stuff**  \n",
    "- train: 9K, test: 1K  \n",
    "- class: 91  \n",
    "\n",
    "### Implementation Details  \n",
    "**Pretraining**  \n",
    "- 위 3개의 dataset에 대해 훈련하기 전에 ImageNet으로 사전 훈련을 한다  \n",
    "- 우리는 D branch를 제거하고 final stage에서 DDRNet과 같은 merging method를 따라 분류 모델을 구성한다  \n",
    "- epoch은 90으로 설정하며 initial learning rate는 0.1, epoch이 30, 60일 때 0.1을 곱한다  \n",
    "- CE Loss와 SGD momentum 0.9를 적용하며 weight decay는 $1e^{-4}$를 적용한다  \n",
    "- $224\\times 224$ 사이즈로 random cropping을 하고 horizontal flip을 적용한다  \n",
    "\n",
    "**Training**  \n",
    "- poly learning rate scheduler를 적용해 매 iteration마다 업데이트한다  \n",
    "- Augmentation  \n",
    "    - random cropping  \n",
    "    - random horizontal flip  \n",
    "    - random scaling in [0.5, 2.0]  \n",
    "    \n",
    "- hyperparameter in Cityscapes  \n",
    "    - epoch : 484  \n",
    "    - initial lr : $1e^{-2}$  \n",
    "    - weight decay : $5e^{-4}$  \n",
    "    - crop size : $1024\\times 1024$  \n",
    "    - batch size : 12  \n",
    "    \n",
    "- hyperparameter in CamVid  \n",
    "    - epoch : 200  \n",
    "    - initial lr : $1e^{-3}$  \n",
    "    - weight decay : $5e^{-4}$  \n",
    "    - crop size : $960\\times 720$  \n",
    "    - batch size : 12  \n",
    "    \n",
    "- hyperparameter in CamVid  \n",
    "    - epoch : 180  \n",
    "    - initial lr : $5e^{-3}$  \n",
    "    - weight decay : $1e^{-4}$  \n",
    "    - crop size : $640\\times 640$  \n",
    "    - batch size : 16  \n",
    "    \n",
    "- over-fitting을 피하기 위해 early stopping도 적용한다   \n",
    "\n",
    "### Ablation Study  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table1.JPG?raw=true\" width=700>\n",
    "\n",
    "**ADB for Two-branch Networks**  \n",
    "- ADB의 효과를 보여주기 위해 PIDNet의 ADB와 Bag를 이용해 BiSeNet과 DDRNet에 적용해 볼 것이다  \n",
    "- mIOU는 향상됐지만 연산량이 늘어나 FPS가 조금 줄어들었다  \n",
    "\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure8.JPG?raw=true\" width=700>\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/figure9.JPG?raw=true\" width=700>\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table2.JPG?raw=true\" width=700>\n",
    "\n",
    "**Collaboration of Pag and Bag**  \n",
    "- element-wise summatation은 feature를 합치는 전통적인 방법이다  \n",
    "- feature를 바로 더하는 다신, 우리는 I branch로부터 유용한 정보를 학습하기 위해 P branch와 Pag module을 제안한다  \n",
    "- 또한 Bag module은 final stage에서 boundary attention을 사용해 detail과 context feature의 결합을 가이드하기 위해 제안됐다  \n",
    "- table 2에서 leteral connection이 모델의 정확도를 개선했음을 보여주고 있다  \n",
    "- 우리는 여러 실험을 통해 Add + Add와 Pag + Bag의 성능을 비교했으며 table 2와 3에서 이를 보여주고 있다  \n",
    "- figure 8에서 첫 번째 Pag에 대한 sigmoid function output을 시각화한 결과, large object보다 small object가 더 어둡게 나왔다  \n",
    "- 또한 boundary region과 small object는 Bag module의 output에서 크게 향상되며 figure 9에서 이를 보여주고 있다  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table4.JPG?raw=true\" width=700>\n",
    "\n",
    "**Efficiency of PAPPM**  \n",
    "- 우리는 PAPPM을 제안했으며 이는 병렬 구조와 적은 parameter 수를 가지고 있다  \n",
    "- table 3에 PAPPM의 효과를 보여주고 있다  \n",
    "- DAPPM과 같은 정확도를 가지면서 9.5 FPS가 증가했다  \n",
    "\n",
    "**Effectiveness of Extra losses**  \n",
    "- 우리는 최적화를 가속화하고 각 component들의 기능을 강조하기 위해 3개의 extra loss를 추가했다  \n",
    "- table 4를 통해 boundary loss $l_1$과 boundary awareness $l_3$는 성능을 위해 꼭 필요함을 알 수 있다  \n",
    "\n",
    "### Comparison  \n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table5.JPG?raw=true\" width=700>\n",
    "\n",
    "**CamVid**  \n",
    "- table 5에서 PIDNet-M 모델이 CamVid에서 가장 높은 정확도를 달성했다  \n",
    "\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table6.JPG?raw=true\" width=700>\n",
    "\n",
    "**Cityscapes**  \n",
    "- Cityscapes에서 PIDNet이 trade-off를 적절히 이루며 SOTA를 달성했다  \n",
    "\n",
    "<img src = \"https://github.com/Sangh0/Segmentation/blob/main/PIDNet/figure/table7.JPG?raw=true\" width=700>\n",
    "\n",
    "**COCO-Stuff**  \n",
    "- COCO-Stuff에서도 마찬가지로 SOTA를 달성했다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695a87f",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "- 본 논문에서 real-time semantic segmentation 모델인 three-branch network인 PIDNet을 제안했다  \n",
    "- PIDNet은 inference speed와 accuracy 사이의 trade-off를 적절히 이뤄냈다  \n",
    "- 그러나 PIDNet은 detailed와 context information의 균형을 맞추기 위해 boundary prediction을 활용했다  \n",
    "- 여기에 성능을 더 높이기 위해선 정밀한 annotation이 필요하다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
